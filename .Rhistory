trace(op)
tr(op)
table(tweets$rescaled)
table(tweets$bow)
sentiment.score$sentiment[sentiment.score$score <= -3] <- 1;
sentiment.score$sentiment[sentiment.score$score == -2 | sentiment.score$score == -1] <- 2;
sentiment.score <- score.sentiment.en(tweets$idd,as.character(tweets$Tweet),pos.words.en,neg.words.en)
sentiment.score$sentiment[sentiment.score$score <= -3] <- 1;
sentiment.score$sentiment[sentiment.score$score == -2 | sentiment.score$score == -1] <- 2;
sentiment.score$sentiment[sentiment.score$score == 0] <- 3;
sentiment.score$sentiment[sentiment.score$score == 1 | sentiment.score$score == 2] <- 4;
sentiment.score$sentiment[sentiment.score$score >= 3] <- 5;
str(sentiment.score)
tweets$bow = sentiment.score$score
op =table(round(tweets$rescaled,0),tweets$bow)
table(tweets$bow)
table(tweets$bow)
str(sentiment.score)
str(sentiment.score$score)
table(sentiment.score$score)
table(sentiment.score$sentiment)
tweets$bow = sentiment.score$sentiment
op =table(round(tweets$rescaled,0),tweets$bow)
table(tweets$bow)
tr(op)
op
matrix.trace(op)
diag(op)
sum(diag(op))
sum(diag(op))/sum(op)
op =table(ceil(tweets$rescaled,0),tweets$bow)
op =table(ceiling(tweets$rescaled,0),tweets$bow)
op =table(ceiling(tweets$rescaled),tweets$bow)
sum(diag(op))/sum(op)
tweets$bow = sentiment.score$sentiment
op =table(floor(tweets$rescaled),tweets$bow)
sum(diag(op))/sum(op)
tweets$bow = sentiment.score$sentiment
op =table(ceiling(tweets$rescaled),tweets$bow)
sum(diag(op))/sum(op)
str(tweets$bow)
str(tweets)
tweets$id <- NULL
str(tweets)
names(tweets)
tweets["alch"] = NULL
names(tweets)
tweets["alch"] = NULL
names(tweets)
tweets["alch"] = 0
names(tweets)
tweets["alch"] = 0
tweets["model"] = 0
names(tweets)
dbWriteTable(con, "twitter.eval_score", tweets,row.names = FALSE)
dbDisconnect(con)
dbSendQuery(con, "DROP TABLE IF EXISTS twitter.eval_score")
dbWriteTable(con, "twitter.eval_score", tweets,row.names = FALSE)
dbDisconnect(con)
login <- fromJSON("/Users/krishna/MOOC/smart-city/login.json", flatten=TRUE)
# loads the PostgreSQL driver
drv <- dbDriver("PostgreSQL")
drv, dbname = login$dbname,
con <- dbConnect(
host = login$host,
port = login$port,
user = login$user,
password = login$password
)
tweets = read.csv("/Users/krishna/MOOC/Edge/Data/tweets.csv")
head(tweets)
linMap <- function(x, from, to)
(x - min(x)) / max(x - min(x)) * (to - from) + from
tweets$rescaled = linMap(tweets$Avg, 1, 5)
str(tweets)
tweets$idd = rownames(tweets)
str(tweets)
pos.words.en <-read.csv('/Users/krishna/MOOC/smart-city/SmartCity/SentimentAnalysisR/english_positive.csv',header=TRUE,sep=",",encoding='UTF-8')
pos.words.en <- pos.words.en[,2]
pos.words.en <- as.character(pos.words.en)
neg.words.en <- read.csv('/Users/krishna/MOOC/smart-city/SmartCity/SentimentAnalysisR/english_negative.csv',header=TRUE,sep=",",encoding='UTF-8')
neg.words.en <- neg.words.en[,2]
neg.words.en <- as.character(neg.words.en)
score.sentiment.en = function(idd, sentences, pos.words, neg.words, .progress='none')
{
require(plyr)
require(stringr)
# we got a vector of sentences. plyr will handle a list or a vector as an "l" for us
# we want a simple array of scores back, so we use "l" + "a" + "ply" = laply:
scores = laply(sentences, function(sentence, pos.words, neg.words) {
Encoding(sentence) <- "UTF-8"
# clean up sentences with R's regex-driven global substitute, gsub():
sentence = gsub('http\\S+\\s*', '', sentence)
sentence = gsub('[[:punct:]]', '', sentence)
sentence = gsub("[\']", " ", sentence)
sentence = gsub('[[:cntrl:]]', '', sentence)
sentence = gsub('\\d+', '', sentence)
sentence = stri_trans_general(sentence ,"Latin-ASCII")
#Clean emojis
sentence <- sapply(sentence, function(row) iconv(row, "latin1", "ASCII", sub=""))
names(sentence) <- NULL
sentence <- gsub("[\n]", " ", sentence)
# replace al \n
# convert to lower case:
# split into words. str_split is in the stringr package
sentence = tolower(sentence)
word.list = str_split(sentence, '\\s+')
# sometimes a list() is one level of hierarchy too much
words = unlist(word.list)
#Stemming
words = wordStem(words, language = "english")
# compare our words to the dictionaries of positive & negative terms
pos.matches = match(words, pos.words.en)
neg.matches = match(words, neg.words.en)
# match() returns the position of the matched term or NA
# we just want a TRUE/FALSE:
pos.matches = !is.na(pos.matches)
neg.matches = !is.na(neg.matches)
# and conveniently enough, TRUE/FALSE will be treated as 1/0 by sum():
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, pos.words, neg.words, .progress=.progress )
scores.df = data.frame(id=idd, score=scores, text=sentences)
return(scores.df)
}
sentiment.score <- score.sentiment.en(tweets$idd,as.character(tweets$Tweet),pos.words.en,neg.words.en)
sentiment.score$sentiment[sentiment.score$score <= -3] <- 1;
sentiment.score$sentiment[sentiment.score$score == -2 | sentiment.score$score == -1] <- 2;
sentiment.score$sentiment[sentiment.score$score == 0] <- 3;
sentiment.score$sentiment[sentiment.score$score == 1 | sentiment.score$score == 2] <- 4;
sentiment.score$sentiment[sentiment.score$score >= 3] <- 5;
table(sentiment.score$sentiment)
tweets$bow = sentiment.score$sentiment
op =table(ceiling(tweets$rescaled),tweets$bow)
sum(diag(op))/sum(op)
tweets["alch"] = 0
tweets["model"] = 0
names(tweets)
tweets = tweets[,c(4,1,2,3,5,6,7)]
str(tweets)
print("Updating tweet scores in DB")
dbSendQuery(con, "DROP TABLE IF EXISTS twitter.tweet_eval")
dbWriteTable(con, "twitter.tweet_eval", tweets,row.names = FALSE)
con <- dbConnect(
drv, dbname = login$dbname,
host = login$host,
port = login$port,
user = login$user,
password = login$password
)
dbWriteTable(con, "twitter.tweet_eval", tweets,row.names = FALSE)
dbCommit(con)
dbWriteTable(con, "twitter.tweet_eval", tweets,row.names = FALSE)
dbWriteTable(con, c("twitter", "tweet_eval"), tweets,row.names = FALSE)
sum(diag(op))/sum(op)
install.packages('gutenbergr')
books <- gutenberg_download(c(768, 1260), meta_fields = "title")
library(gutenbergr)
?gutenberg_download
books
books <- gutenberg_download(c(768, 1260), meta_fields = "title")
books
head(books)
sample(books$gutenberg_id,10)
x = sample(books$gutenberg_id,10)
x
x = sample(books$gutenberg_id)
x
head(books)
setwd("/Users/krishna/Experiment/Text Mining Assignment")
emotions = read.csv("emotionlex.csv")
head(emotions)
unique(emotions$AffectCategory)
unique(emotions$AffectCategory)
x = unique(emotions$AffectCategory)
x
x[1:16]
x[1:10]
books = read.csv("PG2003-08/etext94/80day11.txt")
books = read.csv("/PG2003-08/etext94/80day11.txt")
books = read.table("/PG2003-08/etext94/80day11.txt")
books = read.table("PG2003-08/etext94/80day11.txt")
books = read.table("/Users/krishna/Experiment/Text Mining Assignment/PG2003-08/etext94/80day11.txt")
books <- gutenberg_download(c(1), meta_fields = "title")
head(books)
str(books)
?gutenberg_metadata
gutenberg_metadata(1)
gutenberg_metadata(c(1))
library(NLP)
library(openNLP)
install.packages(openNLP)
install.packages('openNLP')
library(openNLP)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
a3 <- annotate(s,
list(sent_token_annotator,
word_token_annotator,
pos_tag_annotator))
a3 <- annotate(books$text[1],
list(sent_token_annotator,
word_token_annotator,
pos_tag_annotator))
a3
books$text[1]
books$text[2]
books$text[3]
books$text[4]
books$text[10]
books$text[15]
books$text[190]
a3 <- annotate(books$text[190],
list(sent_token_annotator,
word_token_annotator,
pos_tag_annotator))
a3
books$text[190]
m = "I hate yu"
a3 <- annotate(m,
list(sent_token_annotator,
word_token_annotator,
pos_tag_annotator))
a3
a3 <- annotate(m,
list(
word_token_annotator,
pos_tag_annotator))
m = "I hate yu"
a3 <- annotate(m,
list(word_token_annotator,
pos_tag_annotator))
a3 <- annotate(m,
list(sent_token_annotator,
word_token_annotator,
pos_tag_annotator))
a3
m = "I hate you"
a3 <- annotate(m,
list(sent_token_annotator,
word_token_annotator,
pos_tag_annotator))
a3
m = "I hate you. I love you. I wish the product was better quality"
a3 <- annotate(m,
list(sent_token_annotator,
word_token_annotator,
pos_tag_annotator))
a3
p <- parse_annotator(m, a3)
library(NLP)
p <- parse_annotator(m, a3)
s <- paste(c("Pierre Vinken, 61 years old, will join the board as a ",
"nonexecutive director Nov. 29.\n",
"Mr. Vinken is chairman of Elsevier N.V., ",
"the Dutch publishing group."),
collapse = "")
s <- as.String(s)
## Need sentence and word token annotations.
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
a2 <- annotate(s, list(sent_token_annotator, word_token_annotator))
parse_annotator <- Parse_Annotator()
## Compute the parse annotations only.
p <- parse_annotator(s, a2)
a3
q <- strsplit(unlist(3[1]),'/NN')
q <- strsplit(unlist(a3[1]),'/NN')
q
a3 <- annotate(m,
list(sent_token_annotator,
word_token_annotator,
pos_tag_annotator))
a3
a3 <- annotate(m,
list(sent_token_annotator,
,
pos_tag_annotator))
a3
a3 <- annotate(m,
list(sent_token_annotator,
pos_tag_annotator))
a3
a3 <- annotate(m,
list(sent_token_annotator,
pos_tag_annotator))
a3 <- annotate(m,
list(sent_token_annotator,
pos_tag_annotator))
a3 <- annotate(m,
list(sent_token_annotator
))
a3
a3 <- annotate(m,
list(sent_token_annotator,word_token_annotator
))
a3
a3 <- annotate(m,
list(sent_token_annotator,Maxent_POS_Tag_Annotator
))
a3 <- annotate(m,
list(sent_token_annotator,Maxent_POS_Tag_Annotator()
))
a3
a3 <- annotate(m,
list(sent_token_annotator,
word_token_annotator,
pos_tag_annotator))
a3
a3 <- annotate(m,
list(
word_token_annotator,
pos_tag_annotator))
a3 <- annotate(m,
list(sent_token_annotator,
word_token_annotator
))
a3
a3 <- annotate(m,
list(sent_token_annotator,
word_token_annotator,
pos_tag_annotator))
a3
tagged_str <-  tagPOS(a3)
extractPOS <- function(x, thisPOSregex) {
x <- as.String(x)
wordAnnotation <- annotate(x, list(Maxent_Sent_Token_Annotator(), Maxent_Word_Token_Annotator()))
POSAnnotation <- annotate(x, Maxent_POS_Tag_Annotator(), wordAnnotation)
POSwords <- subset(POSAnnotation, type == "word")
tags <- sapply(POSwords$features, '[[', "POS")
thisPOSindex <- grep(thisPOSregex, tags)
tokenizedAndTagged <- sprintf("%s/%s", x[POSwords][thisPOSindex], tags[thisPOSindex])
untokenizedAndTagged <- paste(tokenizedAndTagged, collapse = " ")
untokenizedAndTagged
}
lapply(m, extractPOS, "NN")
a3
lapply(m, extractPOS, "NN,JJ")
lapply(m, extractPOS, "NN")
lapply(m, extractPOS, "JJ")
lapply(m, extractPOS, "VB")
m = "I hate you. I love you. I wish the product was better quality. I am confused with this choics. I love my life and I am in a serious moode"
lapply(m, extractPOS, "VB")
lapply(m, extractPOS, "JJ")
# Adverb
lapply(m, extractPOS, "RB")
lapply(m, extractPOS, "RB")
# Nouns
lapply(m, extractPOS, "NN")
lapply(m, extractPOS, "RB")[2]
m =lapply(m, extractPOS, "NN")
q <- strsplit(unlist(m),'/NN')
q
n =lapply(m, extractPOS, "NN")
q <- strsplit(unlist(m),'/NN')
q
m
q
q <- strsplit(unlist(m),'/NN$')
q
n =lapply(m, extractPOS, "NN")
q <- strsplit(unlist(m),'/NN+')
q
n =lapply(m, extractPOS, "NN")
q <- strsplit(unlist(m),'/NN*')
q
lapply(m, extractPOS, "PRP")
m
n =lapply(m, extractPOS, "NN")
q <- strsplit(unlist(m),'/NNS')
q
n =lapply(m, extractPOS, "NN")
q <- strsplit(unlist(m),'/NN$')
q
q <- strsplit(unlist(m),'/NN{1}')
q
pron = lapply(mTaggedData, extractPOS, "PRP")
pron = lapply(mTaggedData, extractPOS, "JJ")
pron = lapply(m, extractPOS, "JJ")
pron
pron = lapply(m, extractPOS, "VB")
pron
pron = lapply(m, extractPOS, "RB")
pron
m  = "I love you"
pron = lapply(m, extractPOS, "RB")
pron
pron = lapply(m, extractPOS, "NN")
pron
m  = "I love you"
pron = lapply(m, extractPOS, "NN")
pron
pron = lapply(m, extractPOS, "JJ")
pron
k  = "I love you"
pron = lapply(k, extractPOS, "JJ")
pron
txt <- c("This is a short tagging example, by John Doe.",
"Too bad OpenNLP is so slow on large texts.")
pron = lapply(txt, extractPOS, "JJ")
pron
m = c("I love you")
pron = lapply(txt, extractPOS, "JJ")
pron
pron = lapply(m, extractPOS, "JJ")
pron
m = c("I love you so much")
pron = lapply(m, extractPOS, "JJ")
pron
m = c("I love you so much. I hate this homework")
pron = lapply(m, extractPOS, "JJ")
pron
m = c("I love you so much. I hate this homework.")
txt <- c("This is a short tagging example, by John Doe.",
"Too bad OpenNLP is so slow on large texts.")
pron = lapply(m, extractPOS, "JJ")
pron
m = c("I love you so much, I hate this homework.")
txt <- c("This is a short tagging example, by John Doe.",
"Too bad OpenNLP is so slow on large texts.")
pron = lapply(m, extractPOS, "JJ")
pron
m = c("I love you so much, I hate this homework. Krishna had food")
txt <- c("This is a short tagging example, by John Doe.",
"Too bad OpenNLP is so slow on large texts.")
pron = lapply(m, extractPOS, "JJ")
pron
pron = lapply(m, extractPOS, "NN")
pron
emotions = read.csv("emotionlex.csv")
head(emotions)
unique(emotions$AffectCategory)
table(emotions$AffectCategory)
hist(table(emotions$AffectCategory))
plot(table(emotions$AffectCategory))
unique(emotions$AffectCategory)
head(emotions)
anger = subset(emotions,AffectCategory="anger")
anger
head(anger)
anger = subset(emotions,AffectCategory=="anger")
head(anger)
anger = subset(emotions,AffectCategory=="anger",AssociationFlag=1)
head(anger)
anger = subset(emotions,AffectCategory=="anger",AssociationFlag=="1")
head(anger)
anger = subset(emotions,AffectCategory=="anger",AssociationFlag==1)
head(anger)
anger = subset(emotions,AffectCategory=="anger",AssociationFlag=1)
head(anger)
anger = subset(emotions,AffectCategory=="anger"&&AssociationFlag=1)
anger = subset(emotions,AffectCategory=="anger"&AssociationFlag=1)
anger = subset(emotions,AffectCategory=="anger" & AssociationFlag=1)
anger = subset(emotions,AffectCategory=="anger" & AssociationFlag==1)
head(anger)
head(emotions)
unique(emotions$AffectCategory)
anticipation = subset(emotions,AffectCategory=="anticipation" & AssociationFlag==1)
disgust = subset(emotions,AffectCategory=="disgust" & AssociationFlag==1)
fear = subset(emotions,AffectCategory=="fear" & AssociationFlag==1)
anger = subset(emotions,AffectCategory=="anger" & AssociationFlag==1)
anticipation = subset(emotions,AffectCategory=="anticipation" & AssociationFlag==1)
disgust = subset(emotions,AffectCategory=="disgust" & AssociationFlag==1)
fear = subset(emotions,AffectCategory=="fear" & AssociationFlag==1)
joy = subset(emotions,AffectCategory=="joy" & AssociationFlag==1)
sadness = subset(emotions,AffectCategory=="sadness" & AssociationFlag==1)
surprise = subset(emotions,AffectCategory=="surprise" & AssociationFlag==1)
trust = subset(emotions,AffectCategory=="trust" & AssociationFlag==1)
anger = unlist(anger$TargetWord)
str(anger)
head(anger)
head(anger)
anger = subset(emotions,AffectCategory=="anger" & AssociationFlag==1)
head(anger)
tail(anger)
dim(anger)
anger = unlist(anger$TargetWord)
head(anger)
dim(anger)
lenght(anger)
length(anger)
anger = subset(emotions,AffectCategory=="anger" & AssociationFlag==1)
anger = unlist(anger$TargetWord)
anticipation = subset(emotions,AffectCategory=="anticipation" & AssociationFlag==1)
anticipation = unlist(anticipation$TargetWord)
disgust = subset(emotions,AffectCategory=="disgust" & AssociationFlag==1)
disgust = unlist(disgust$TargetWord)
fear = subset(emotions,AffectCategory=="fear" & AssociationFlag==1)
fear = unlist(fear$TargetWord)
joy = subset(emotions,AffectCategory=="joy" & AssociationFlag==1)
joy = unlist(joy$TargetWord)
sadness = subset(emotions,AffectCategory=="sadness" & AssociationFlag==1)
sadness = unlist(sadness$TargetWord)
surprise = subset(emotions,AffectCategory=="surprise" & AssociationFlag==1)
surprise = unlist(surprise$TargetWord)
trust = subset(emotions,AffectCategory=="trust" & AssociationFlag==1)
trust = unlist(trust$TargetWord)
head(surprise)
head(sadness)
str(trust)
summary(trust)
pron@TT.res
txt <- c("This is a short tagging example, by John Doe.",
"Too bad OpenNLP is so slow on large texts.")
pron = lapply(m, extractPOS, "NN")
txt <- c("This is a short tagging example, by John Doe.",
"Too bad OpenNLP is so slow on large texts.")
proNoun = lapply(m, extractPOS, "NN")
proNoun
proNoun = lapply(txt, extractPOS, "NN")
proNoun
